{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_deriv(f, x, i):\n",
    "    # TODO: vectorize\n",
    "    eps = np.zeros(len(x))\n",
    "    eps[i] = 0.000001\n",
    "    deriv = (f(x+eps) - f(x-eps)) / (2*eps[i])\n",
    "    return deriv\n",
    "\n",
    "def gradient(f, x):\n",
    "    # gradient of f at x\n",
    "    g = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        g[i] = partial_deriv(f, x, i)\n",
    "    return g.T\n",
    "\n",
    "def hessian(f, x):\n",
    "    gradient_functions = [partial(lambda x, idx: gradient(f, x)[idx], idx=i) for i in range(len(x))]\n",
    "    hesse_matrix = np.array([gradient(g, x) for g in gradient_functions])\n",
    "    return hesse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(L, D, x, alpha):\n",
    "    # For Mini-Batch: \n",
    "    # x: parameters\n",
    "    # L: loss function\n",
    "    # D: Data (n*m)\n",
    "    # alpha: Lernrate\n",
    "\n",
    "    eps  = 0.01\n",
    "\n",
    "    nit = 0\n",
    "    f = lambda x: L(D[:, 0], D[:, 1], x)\n",
    "    G = gradient(f, x)\n",
    "    d = -G\n",
    "    print(G)\n",
    "    while np.sum(np.abs(G)) >= eps:\n",
    "\n",
    "        # Construct loss function based on a subset of observations\n",
    "        random_idx = np.random.choice(D.shape[0], size=int(0.1*D.shape[0]))\n",
    "        f = lambda x: L(D[random_idx, 0], D[random_idx, 1], x)\n",
    "        \n",
    "        x = x + alpha*d\n",
    "        print(f\"f={f(x)} at x={x}\")\n",
    "        G = gradient(f, x)\n",
    "        d = -G   \n",
    "        nit += 1\n",
    "        print(f\"Iteration: {nit}, Gradient: {G}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, Y, theta):\n",
    "    loss = np.mean(np.square((theta[0] + theta[1]*X) - Y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = pd.read_table(\"Serie07_Daten/x_values.txt\").values.flatten()\n",
    "y_values = x_values = pd.read_table(\"Serie07_Daten/y_values.txt\").values.flatten()\n",
    "df = pd.DataFrame.from_dict({\"X\":x_values, \"Y\":y_values})\n",
    "df = df.dropna()\n",
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449932, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(df[:, 0], df[:, 1], (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99931393 -0.01971077]\n",
      "f=0.4202049452273532 at x=[0.90006861 0.00197108]\n",
      "Iteration: 1, Gradient: [ 0.80029496 -0.1203267 ]\n",
      "f=0.3597540945780829 at x=[0.82003911 0.01400375]\n",
      "Iteration: 2, Gradient: [ 0.65231391 -0.18720687]\n",
      "f=0.3205160893341728 at x=[0.75480772 0.03272443]\n",
      "Iteration: 3, Gradient: [ 0.54926973 -0.23409993]\n",
      "f=0.27874643537682314 at x=[0.69988075 0.05613443]\n",
      "Iteration: 4, Gradient: [ 0.45196843 -0.25551188]\n",
      "f=0.25482130312548495 at x=[0.6546839  0.08168561]\n",
      "Iteration: 5, Gradient: [ 0.39407476 -0.27403274]\n",
      "f=0.2356594666881044 at x=[0.61527643 0.10908889]\n",
      "Iteration: 6, Gradient: [ 0.33857004 -0.29520876]\n",
      "f=0.21616230866276942 at x=[0.58141942 0.13860976]\n",
      "Iteration: 7, Gradient: [ 0.29670867 -0.30161989]\n",
      "f=0.19645409696742325 at x=[0.55174856 0.16877175]\n",
      "Iteration: 8, Gradient: [ 0.27029407 -0.29326943]\n",
      "f=0.18010511194459772 at x=[0.52471915 0.1980987 ]\n",
      "Iteration: 9, Gradient: [ 0.24094012 -0.29153753]\n",
      "f=0.16753459119152414 at x=[0.50062514 0.22725245]\n",
      "Iteration: 10, Gradient: [ 0.22762104 -0.28614308]\n",
      "f=0.15356055781704456 at x=[0.47786303 0.25586676]\n",
      "Iteration: 11, Gradient: [ 0.21023041 -0.27771878]\n",
      "f=0.14259981599887958 at x=[0.45683999 0.28363864]\n",
      "Iteration: 12, Gradient: [ 0.19686947 -0.27257442]\n",
      "f=0.13009231406941496 at x=[0.43715304 0.31089608]\n",
      "Iteration: 13, Gradient: [ 0.18434394 -0.26062559]\n",
      "f=0.12281501155311823 at x=[0.41871865 0.33695864]\n",
      "Iteration: 14, Gradient: [ 0.18105953 -0.25611829]\n",
      "f=0.11217254542511922 at x=[0.4006127  0.36257047]\n",
      "Iteration: 15, Gradient: [ 0.1635299  -0.24917725]\n",
      "f=0.10286335693221338 at x=[0.38425971 0.38748819]\n",
      "Iteration: 16, Gradient: [ 0.14998385 -0.2417814 ]\n",
      "f=0.09551397092936056 at x=[0.36926132 0.41166633]\n",
      "Iteration: 17, Gradient: [ 0.14832807 -0.23159667]\n",
      "f=0.08849312970699663 at x=[0.35442851 0.434826  ]\n",
      "Iteration: 18, Gradient: [ 0.13998438 -0.22536741]\n",
      "f=0.08065986170961383 at x=[0.34043008 0.45736274]\n",
      "Iteration: 19, Gradient: [ 0.13395792 -0.2132482 ]\n",
      "f=0.07513867074434293 at x=[0.32703429 0.47868756]\n",
      "Iteration: 20, Gradient: [ 0.12689727 -0.20866102]\n",
      "f=0.06887976778080286 at x=[0.31434456 0.49955366]\n",
      "Iteration: 21, Gradient: [ 0.12742786 -0.19523228]\n",
      "f=0.06348077377155492 at x=[0.30160177 0.51907689]\n",
      "Iteration: 22, Gradient: [ 0.12379702 -0.18635858]\n",
      "f=0.05889472935441888 at x=[0.28922207 0.53771275]\n",
      "Iteration: 23, Gradient: [ 0.1159341  -0.18226494]\n",
      "f=0.054133501877093135 at x=[0.27762866 0.55593924]\n",
      "Iteration: 24, Gradient: [ 0.10931118 -0.17546943]\n",
      "f=0.05016856778228438 at x=[0.26669754 0.57348618]\n",
      "Iteration: 25, Gradient: [ 0.107264   -0.16817765]\n",
      "f=0.04656699154889136 at x=[0.25597114 0.59030395]\n",
      "Iteration: 26, Gradient: [ 0.10272942 -0.16314098]\n",
      "f=0.042465435812060214 at x=[0.2456982  0.60661805]\n",
      "Iteration: 27, Gradient: [ 0.09829485 -0.15450634]\n",
      "f=0.03941541498977911 at x=[0.23586872 0.62206868]\n",
      "Iteration: 28, Gradient: [ 0.09460051 -0.14954444]\n",
      "f=0.036666958715988136 at x=[0.22640866 0.63702313]\n",
      "Iteration: 29, Gradient: [ 0.09290144 -0.14408694]\n",
      "f=0.033714693692812894 at x=[0.21711852 0.65143182]\n",
      "Iteration: 30, Gradient: [ 0.08453631 -0.14079022]\n",
      "f=0.030857370608836726 at x=[0.20866489 0.66551084]\n",
      "Iteration: 31, Gradient: [ 0.07916548 -0.13511854]\n",
      "f=0.02817033275975183 at x=[0.20074834 0.67902269]\n",
      "Iteration: 32, Gradient: [ 0.08104643 -0.12483976]\n",
      "f=0.02631520344385183 at x=[0.1926437  0.69150667]\n",
      "Iteration: 33, Gradient: [ 0.07946591 -0.1209809 ]\n",
      "f=0.024317114172301025 at x=[0.18469711 0.70360476]\n",
      "Iteration: 34, Gradient: [ 0.0736738  -0.11817629]\n",
      "f=0.02221145798659962 at x=[0.17732973 0.71542239]\n",
      "Iteration: 35, Gradient: [ 0.06839456 -0.11348232]\n",
      "f=0.020522995518020646 at x=[0.17049027 0.72677062]\n",
      "Iteration: 36, Gradient: [ 0.06637327 -0.10880966]\n",
      "f=0.019013599639195952 at x=[0.16385294 0.73765159]\n",
      "Iteration: 37, Gradient: [ 0.06466144 -0.10456412]\n",
      "f=0.01738585825400465 at x=[0.1573868 0.748108 ]\n",
      "Iteration: 38, Gradient: [ 0.06391163 -0.09810899]\n",
      "f=0.01615991755878999 at x=[0.15099564 0.7579189 ]\n",
      "Iteration: 39, Gradient: [ 0.0599308  -0.09612707]\n",
      "f=0.014861099692120634 at x=[0.14500256 0.7675316 ]\n",
      "Iteration: 40, Gradient: [ 0.0557936  -0.09305344]\n",
      "f=0.013852643647623345 at x=[0.1394232  0.77683695]\n",
      "Iteration: 41, Gradient: [ 0.05627677 -0.08898874]\n",
      "f=0.012726693588523961 at x=[0.13379552 0.78573582]\n",
      "Iteration: 42, Gradient: [ 0.05413777 -0.08498853]\n",
      "f=0.011681455981667973 at x=[0.12838174 0.79423468]\n",
      "Iteration: 43, Gradient: [ 0.05089789 -0.08178517]\n",
      "f=0.010660631471363468 at x=[0.12329195 0.80241319]\n",
      "Iteration: 44, Gradient: [ 0.04784599 -0.07805297]\n",
      "f=0.009952779351155546 at x=[0.11850736 0.81021849]\n",
      "Iteration: 45, Gradient: [ 0.04669254 -0.07572998]\n",
      "f=0.009160670185609609 at x=[0.1138381  0.81779149]\n",
      "Iteration: 46, Gradient: [ 0.04497605 -0.0724519 ]\n",
      "f=0.008448615081314873 at x=[0.1093405  0.82503668]\n",
      "Iteration: 47, Gradient: [ 0.04323985 -0.0695538 ]\n",
      "f=0.007740288071819712 at x=[0.10501651 0.83199206]\n",
      "Iteration: 48, Gradient: [ 0.04115564 -0.06641682]\n",
      "f=0.007129694130110253 at x=[0.10090095 0.83863374]\n",
      "Iteration: 49, Gradient: [ 0.03936236 -0.06375365]\n",
      "f=0.00665182037682269 at x=[0.09696471 0.8450091 ]\n",
      "Iteration: 50, Gradient: [ 0.03899389 -0.0614398 ]\n",
      "f=0.006105887537074684 at x=[0.09306532 0.85115308]\n",
      "Iteration: 51, Gradient: [ 0.03740839 -0.05865322]\n",
      "f=0.005602238632862561 at x=[0.08932448 0.85701841]\n",
      "Iteration: 52, Gradient: [ 0.03504065 -0.05647223]\n",
      "f=0.005168079690291412 at x=[0.08582042 0.86266563]\n",
      "Iteration: 53, Gradient: [ 0.03469203 -0.05358364]\n",
      "f=0.004774842531573894 at x=[0.08235122 0.86802399]\n",
      "Iteration: 54, Gradient: [ 0.03267762 -0.05196886]\n",
      "f=0.004460846078017414 at x=[0.07908345 0.87322088]\n",
      "Iteration: 55, Gradient: [ 0.03131157 -0.05084012]\n",
      "f=0.0040690945502290885 at x=[0.0759523  0.87830489]\n",
      "Iteration: 56, Gradient: [ 0.02914754 -0.04868204]\n",
      "f=0.0037415331029246423 at x=[0.07303754 0.8831731 ]\n",
      "Iteration: 57, Gradient: [ 0.02890078 -0.04598448]\n",
      "f=0.00346387901665513 at x=[0.07014747 0.88777154]\n",
      "Iteration: 58, Gradient: [ 0.02853867 -0.04389121]\n",
      "f=0.003206302367382154 at x=[0.0672936  0.89216066]\n",
      "Iteration: 59, Gradient: [ 0.02656673 -0.04288633]\n",
      "f=0.002948303236300114 at x=[0.06463693 0.8964493 ]\n",
      "Iteration: 60, Gradient: [ 0.02630778 -0.04052268]\n",
      "f=0.0026845671700546944 at x=[0.06200615 0.90050157]\n",
      "Iteration: 61, Gradient: [ 0.02437319 -0.03877294]\n",
      "f=0.002538439855833994 at x=[0.05956883 0.90437886]\n",
      "Iteration: 62, Gradient: [ 0.02307493 -0.03871877]\n",
      "f=0.00231723708305544 at x=[0.05726134 0.90825074]\n",
      "Iteration: 63, Gradient: [ 0.02360149 -0.03578254]\n",
      "f=0.0021126782393978143 at x=[0.05490119 0.91182899]\n",
      "Iteration: 64, Gradient: [ 0.02152554 -0.03451904]\n",
      "f=0.001977111448596547 at x=[0.05274863 0.9152809 ]\n",
      "Iteration: 65, Gradient: [ 0.02092764 -0.03364434]\n",
      "f=0.0018184756430370396 at x=[0.05065587 0.91864533]\n",
      "Iteration: 66, Gradient: [ 0.0207243 -0.0318008]\n",
      "f=0.0016778637443794525 at x=[0.04858344 0.92182541]\n",
      "Iteration: 67, Gradient: [ 0.0191399  -0.03103112]\n",
      "f=0.0015376308401857611 at x=[0.04666945 0.92492852]\n",
      "Iteration: 68, Gradient: [ 0.01810067 -0.02971186]\n",
      "f=0.0014197856252875416 at x=[0.04485938 0.92789971]\n",
      "Iteration: 69, Gradient: [ 0.01720775 -0.02867731]\n",
      "f=0.0013139062077253973 at x=[0.04313861 0.93076744]\n",
      "Iteration: 70, Gradient: [ 0.01702009 -0.02735114]\n",
      "f=0.0012130214869200478 at x=[0.0414366  0.93350255]\n",
      "Iteration: 71, Gradient: [ 0.01590707 -0.02657106]\n",
      "f=0.0011185307086408701 at x=[0.03984589 0.93615966]\n",
      "Iteration: 72, Gradient: [ 0.01627827 -0.02488143]\n",
      "f=0.0010241554139675325 at x=[0.03821806 0.9386478 ]\n",
      "Iteration: 73, Gradient: [ 0.01522864 -0.02389974]\n",
      "f=0.0009640515072077212 at x=[0.0366952  0.94103778]\n",
      "Iteration: 74, Gradient: [ 0.01447008 -0.02369518]\n",
      "f=0.0008821220763121586 at x=[0.03524819 0.9434073 ]\n",
      "Iteration: 75, Gradient: [ 0.01432486 -0.02225232]\n",
      "f=0.0008143112298463341 at x=[0.03381571 0.94563253]\n",
      "Iteration: 76, Gradient: [ 0.01339348 -0.02162529]\n",
      "f=0.0007489410382798485 at x=[0.03247636 0.94779506]\n",
      "Iteration: 77, Gradient: [ 0.01261893 -0.02084219]\n",
      "f=0.000689086604140482 at x=[0.03121446 0.94987927]\n",
      "Iteration: 78, Gradient: [ 0.01222363 -0.01988437]\n",
      "f=0.0006386502879310577 at x=[0.0299921  0.95186771]\n",
      "Iteration: 79, Gradient: [ 0.01163356 -0.01928821]\n",
      "f=0.0005874454836193328 at x=[0.02882874 0.95379653]\n",
      "Iteration: 80, Gradient: [ 0.01191703 -0.01799298]\n",
      "f=0.0005332583092659074 at x=[0.02763704 0.95559583]\n",
      "Iteration: 81, Gradient: [ 0.01095136 -0.01720229]\n",
      "f=0.0005004118906494955 at x=[0.02654191 0.95731606]\n",
      "Iteration: 82, Gradient: [ 0.01012352 -0.01715227]\n",
      "f=0.00046107130651223813 at x=[0.02552955 0.95903129]\n",
      "Iteration: 83, Gradient: [ 0.00991011 -0.016333  ]\n",
      "f=0.00042666207370227455 at x=[0.02453854 0.96066459]\n",
      "Iteration: 84, Gradient: [ 0.00981297 -0.01557192]\n",
      "f=0.00039761780346975857 at x=[0.02355725 0.96222178]\n",
      "Iteration: 85, Gradient: [ 0.00932259 -0.01523685]\n",
      "f=0.00036275594522611334 at x=[0.02262499 0.96374546]\n",
      "Iteration: 86, Gradient: [ 0.0089453  -0.01442922]\n",
      "f=0.00033445702860441037 at x=[0.02173046 0.96518839]\n",
      "Iteration: 87, Gradient: [ 0.00896185 -0.013621  ]\n",
      "f=0.00030669940584961483 at x=[0.02083427 0.96655049]\n",
      "Iteration: 88, Gradient: [ 0.00845278 -0.01307317]\n",
      "f=0.00028318650297666194 at x=[0.01998899 0.9678578 ]\n",
      "Iteration: 89, Gradient: [ 0.00776698 -0.01279063]\n",
      "f=0.00026140044011079954 at x=[0.01921229 0.96913687]\n",
      "Iteration: 90, Gradient: [ 0.00774635 -0.01211723]\n",
      "f=0.00024280287965815344 at x=[0.01843766 0.97034859]\n",
      "Iteration: 91, Gradient: [ 0.00734888 -0.01180752]\n",
      "f=0.00022254623746043918 at x=[0.01770277 0.97152934]\n",
      "Iteration: 92, Gradient: [ 0.0067283  -0.01144978]\n",
      "f=0.00020664128886039687 at x=[0.01702994 0.97267432]\n",
      "Iteration: 93, Gradient: [ 0.0067907  -0.01089222]\n",
      "f=0.00018954983650462545 at x=[0.01635087 0.97376354]\n",
      "Iteration: 94, Gradient: [ 0.00641167 -0.01045352]\n",
      "f=0.00017601490599451313 at x=[0.0157097  0.97480889]\n",
      "Iteration: 95, Gradient: [ 0.00629232 -0.01005035]\n",
      "f=0.00016183502255782216 at x=[0.01508047 0.97581393]\n",
      "Iteration: 96, Gradient: [ 0.00607034 -0.00959753]\n",
      "f=0.00014889063585624542 at x=[0.01447344 0.97677368]\n",
      "Iteration: 97, Gradient: [ 0.00582172 -0.00919306]\n",
      "f=0.0001365974337278892 at x=[0.01389127 0.97769299]\n",
      "Iteration: 98, Gradient: [ 0.00570169 -0.00869642]\n",
      "f=0.00012757800018255036 at x=[0.0133211  0.97856263]\n",
      "Iteration: 99, Gradient: [ 0.00528109 -0.00862074]\n",
      "f=0.0001163865489249509 at x=[0.01279299 0.9794247 ]\n",
      "Iteration: 100, Gradient: [ 0.00495808 -0.00823047]\n",
      "f=0.0001068019855929682 at x=[0.01229718 0.98024775]\n",
      "Iteration: 101, Gradient: [ 0.00490345 -0.00776141]\n",
      "f=9.860204836441923e-05 at x=[0.01180684 0.98102389]\n",
      "Iteration: 102, Gradient: [ 0.00463013 -0.00751139]\n",
      "f=9.20854516127387e-05 at x=[0.01134382 0.98177503]\n",
      "Iteration: 103, Gradient: [ 0.00458765 -0.00724991]\n",
      "f=8.478773833144652e-05 at x=[0.01088506 0.98250002]\n",
      "Iteration: 104, Gradient: [ 0.00430171 -0.00701436]\n",
      "f=7.721851159290383e-05 at x=[0.01045489 0.98320146]\n",
      "Iteration: 105, Gradient: [ 0.00398194 -0.00671524]\n",
      "f=7.246675558856794e-05 at x=[0.01005669 0.98387298]\n",
      "Iteration: 106, Gradient: [ 0.00393444 -0.00653351]\n",
      "f=6.650065482351967e-05 at x=[0.00966325 0.98452633]\n",
      "Iteration: 107, Gradient: [ 0.00391983 -0.00614741]\n",
      "f=6.0798603778920107e-05 at x=[0.00927127 0.98514107]\n",
      "Iteration: 108, Gradient: [ 0.00364046 -0.00591197]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00927127, 0.98514107])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_gradient_descent(loss_function, df, (1,0), 0.1) # Schrittweite ist sehr wichtig..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
